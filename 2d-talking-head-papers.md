# 2025

Playmate: Flexible Control of Portrait Animation via 3D-Implicit Space Guided Diffusion; [arXiv 2025](https://arxiv.org/abs/2502.07203); [Project](https://playmate111.github.io/); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145622964); 

OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models; [arXiv 2025](https://arxiv.org/abs/2502.01061); [Project](https://omnihuman-lab.github.io/); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145502287); 

SyncAnimation: A Real-Time End-to-End Framework for Audio-Driven Human Pose and Talking Head Animation; [arXiv 2025](https://arxiv.org/pdf/2501.14646); [Project](https://syncanimation.github.io/); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145406554); 

EMO2: End-Effector Guided Audio-Driven Avatar Video Generation; [arXiv 2025](https://arxiv.org/pdf/2501.10687); [Project](https://humanaigc.github.io/emote-portrait-alive-2/); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145331492); 

Joint Learning of Depth and Appearance for Portrait Image Animation; [arXiv 2025](https://arxiv.org/abs/2501.08649); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145331590); 

UniAvatar: Taming Lifelike Audio-Driven Talking Head Generation with Comprehensive Motion and Lighting Control;  [arXiv 2025](https://www.arxiv.org/abs/2412.19860); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145077676); 

MoEE: Mixture of Emotion Experts for Audio-Driven Portrait Animation;  [arXiv 2025](https://arxiv.org/abs/2501.01808); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145044690); 

VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior; [3DV 2025](https://arxiv.org/abs/2312.01841); [Project](https://humanaigc.github.io/vivid-talk/); [Code](https://github.com/HumanAIGC/VividTalk); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145560934); 

INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations; [arXiv 2025](https://arxiv.org/abs/2412.04037); [Project](https://grisoon.github.io/INFP/); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/144383953); 

# 2024

X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention; [SIGGRAPH 2024](https://arxiv.org/abs/2403.15931); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145692176); 

LES-Talker: Fine-Grained Emotion Editing for Talking Head Generation in Linear Emotion Space; [arXiv 2024](https://arxiv.org/abs/2411.09268); [Project](https://peterfanfan.github.io/LES-Talker/); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145016470); 

Synergizing Motion and Appearance: Multi-Scale Compensatory Codebooks for Talking Head Video Generation; [arXiv 2024](https://arxiv.org/abs/2412.00719); [Project](https://shaelynz.github.io/synergize-motion-appearance/); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145016401); 

VQTalker: Towards Multilingual Talking Avatars through Facial Motion Tokenization; [arXiv 2024](https://arxiv.org/abs/2412.09892); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/144917517); 

Real-time One-Step Diffusion-based Expressive Portrait Videos Generation; [arXiv 2024](https://arxiv.org/abs/2412.13479); [Project](https://guohanzhong.github.io/osalcm/); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/144917395); 

PortraitTalk: Towards Customizable One-Shot Audio-to-Talking Face Generation; [arXiv 2024](https://arxiv.org/abs/2412.07754); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/144627851); 

EmotiveTalk: Expressive Talking Head Generation through Audio Information Decoupling and Emotional Video Diffusion; [arXiv 2024](https://arxiv.org/abs/2411.16726); [Project](https://emotivetalk.github.io/); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/144627903); 

LatentSync: Audio Conditioned Latent Diffusion Models for Lip Sync; [arXiv 2024](https://arxiv.org/abs/2412.09262);  [Code](https://github.com/bytedance/LatentSync); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/144568534); 

LetsTalk: Latent Diffusion Transformer for Talking Video Synthesis; [arXiv 2024](https://arxiv.org/abs/2411.16748); [Project](https://zhang-haojie.github.io/project-pages/letstalk.html); [Code](https://github.com/zhang-haojie/letstalk); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/144567850); 

Sonic: Shifting Focus to Global Audio Perception in Portrait Animation; [arXiv 2024](https://arxiv.org/abs/2411.16331); [Project](https://jixiaozhong.github.io/Sonic/); [Code](https://github.com/jixiaozhong/Sonic); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/144461131); 

IF-MDM: Implicit Face Motion Diffusion Model for High-Fidelity Realtime Talking Head Generation; [arXiv 2024](https://arxiv.org/abs/2412.04000); [Project](http://ec2-3-25-102-128.ap-southeast-2.compute.amazonaws.com/IF-MDM/ifmdm_supplementary/index.html); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/144437117); 

PersonaTalk: Bring Attention to Your Persona in Visual Dubbing; [SIGGRAPH Asia 2024](https://arxiv.org/abs/2409.05379); [Project](https://grisoon.github.io/PersonaTalk/); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/143242962); 

TALK-Act: Enhance Textural-Awareness for 2D Speaking Avatar Reenactment with Diffusion Model; [SIGGRAPH Asia 2024](https://arxiv.org/abs/2410.10696); [Project](https://guanjz20.github.io/projects/TALK-Act/); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/143261671); 

Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation; [arXiv 2024](https://arxiv.org/abs/2406.08801); [Project](https://fudan-generative-vision.github.io/hallo/#/); [Code](https://github.com/fudan-generative-vision/hallo); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/143418507); 

EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions; [arXiv 2024](https://arxiv.org/abs/2402.17485); [Project](https://humanaigc.github.io/emote-portrait-alive/); [Code](https://github.com/HumanAIGC/EMO); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145623045); 

MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation; [arXiv 2024](https://arxiv.org/abs/2412.04448); [Project](https://memoavatar.github.io/); [Code](https://github.com/memoavatar/memo); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/144384770); 

LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control; [arXiv 2024](https://arxiv.org/abs/2407.03168); [Code](https://github.com/KwaiVGI/LivePortrait); [Project](https://liveportrait.github.io/); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145692217); 

# 2023

DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation; [CVPR 2023](https://arxiv.org/abs/2301.03786); [Project](https://sstzal.github.io/DiffTalk/); [Code](https://github.com/sstzal/DiffTalk); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145643635); 

SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation; [CVPR 2023](https://arxiv.org/abs/2211.12194); [Code](https://github.com/Winfredy/SadTalker); [Project](https://sadtalker.github.io/); [CSDN](https://mp.csdn.net/mp_blog/creation/success/145406735); 

# 2022

EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model; [SIGGRAPH 2022](https://arxiv.org/abs/2205.15278); [Code](https://github.com/jixinya/EAMM); [Project](https://jixinya.github.io/projects/EAMM/); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145559700); 

Implicit Warping for Animation with Image Sets; [NeurIPS 2022](https://arxiv.org/abs/2210.01794); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/143261310); 

Depth-Aware Generative Adversarial Network for Talking Head Video Generation; [CVPR 2022](https://arxiv.org/abs/2203.06605); [Project](https://harlanhong.github.io/publications/dagan.html); [Code](https://github.com/harlanhong/CVPR2022-DaGAN); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145691993); 

Thin-Plate Spline Motion Model for Image Animation; [CVPR 2022](https://arxiv.org/abs/2203.14367); [Code](https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145692130); 

# 2021

Audio-Driven Emotional Video Portraits; [CVPR 2021](https://arxiv.org/abs/2104.07452); [Code](https://github.com/jixinya/EVP/); [Project](https://jixinya.github.io/projects/evp/); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145559729); 

Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation; [ACM TOG 2021](https://arxiv.org/abs/2109.10595); [Code](https://github.com/YuanxunLu/LiveSpeechPortraits); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145560915); 

# 2020

A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild; [ACMMM 2020](https://arxiv.org/abs/2008.10010); [Code](https://github.com/Rudrabha/Wav2Lip); [Project](http://cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild/); [CSDN](https://mp.csdn.net/mp_blog/creation/success/145406697); 

MakeItTalk: Speaker-Aware Talking-Head Animation; [SIGGRAPH Asia 2020](https://arxiv.org/abs/2004.12992); [Project](https://people.umass.edu/~yangzhou/MakeItTalk/); [Code](https://github.com/yzhou359/MakeItTalk); [CSDN](https://blog.csdn.net/A_D_I_D_A_S/article/details/145692071); 